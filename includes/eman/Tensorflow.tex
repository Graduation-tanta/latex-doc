\section{TensorFlow}
\label{chap:TensorFlow}

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % SUBSECTION               %
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Introduction}
    The Google Brain project started in 2011 to explore the use of very-large-scale deep neural networks, both for research and for use in Google’s products. As part of the early work in this project, they built DistBelief, their first-generation scalable distributed training and inference system, and this system has served them well. They and others at Google have performed a wide variety of research using DistBelief including work on unsupervised learning, language representation, models for image classification and object detection, video classification, speech recognition, sequence prediction, move selection for Go, pedestrian detection, reinforcement learning, and other areas. In addition, often in close collaboration with the Google Brain team, more than 50 teams at Google and other Alphabet companies have deployed deep neural networks using DistBelief in a wide variety of products, including Google Search, Their advertising products, speech recognition systems, Google Photos, Google Maps and StreetView, Google Translate, YouTube, and many others.\\
    Based on their experience with DistBelief and a more complete understanding of the desirable system properties and requirements for training and using neural networks, they have built TensorFlow, their second-generation system for the implementation and deployment of largescale machine learning models. TensorFlow takes computations described using a dataflow-like model and maps them onto a wide variety of different hardware platforms, ranging from running inference on mobile device platforms such as Android and iOS to modestsized training and inference systems using single machines containing one or many GPU cards to large-scale training systems running on hundreds of specialized machines with thousands of GPUs. Having a single system that can span such a broad range of platforms significantly simplifies the real-world use of machine learning system, as they have found that having separate systems for large-scale training and small-scale deployment leads to significant maintenance burdens and leaky abstractions.\\
    TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery.\\
    TensorFlow is an open-source software library for dataflow programming across a range of tasks, and is arguably one of the best deep learning frameworks and has been adopted by several giants such as Airbus, Twitter, IBM, and others mainly due to its highly flexible system architecture. It is available on both desktop and mobile and also supports languages such as Python, C++, and R to create deep learning models along with wrapper libraries.\\
    \subsubsection{CUDA and TensorFlow}
    CUDA or Compute Unified Device Architecture is an API for Parallel Computing by NVIDIA. CUDA allows you to harness the power of your GPU for computing applications. They (GPUs) can do parallel processing much faster/better than traditional chips (CPUs).\\
    Now, if you've gone through the install docs for Tensorflow using NVIDIA GPUs, you will notice they ask you to install CUDA libraries and toolkits first. It is only then, that Tensorflow can harness the power and speed of computation that your GPU would offer you.\\
    Tensorflow will access the better computation power of NVIDIA GPUs via (or using, both terms are interchangeable here) the CUDA API.\\
    You can absolutely perform tensorflow’s computations from GPUs, but in case you're using an NVIDIA GPU, you need to have CUDA installed so that Tensorflow can actually access what that awesome NVIDIA GPU has to offer with all its parallel computing capable, CUDA Cores.\\
    \subsection{Tensors}
    TensorFlow, as the name indicates, is a framework to define and run computations involving tensors. A tensor is a generalization of vectors and matrices to potentially higher dimensions. Internally, TensorFlow represents tensors as n-dimensional arrays of base datatypes.\\
    When writing a TensorFlow program, the main object you manipulate and pass around is the. A \textbf{tf.Tensor} object represents a partially defined computation that will eventually produce a value. TensorFlow programs work by first building a graph of \textbf{tf.Tensor} objects, detailing how each tensor is computed based on the other available tensors and then by running parts of this graph to achieve the desired results.\\
    A \textbf{tf.Tensor} has the following properties:
    \begin{enumerate}
      \item\textcite{a data type (float32, int32, or string, for example)}
      \item\textcite{a shape}
      \end{enumerate}
      Each element in the Tensor has the same data type, and the data type is always known. The shape (that is, the number of dimensions it has and the size of each dimension) might be only partially known. Most operations produce tensors of fully-known shapes if the shapes of their inputs are also fully known, but in some cases it's only possible to find the shape of a tensor at graph execution time.\\
      The main types of tensors are:
    \subsubsection{Types of tensors}
      \begin{enumerate}
    \item\textcite{tf.Variable}
    \item\textcite{tf.constant}
    \item\textcite{tf.placeholder}
    \end{enumerate}
    \paragraph{Variables}
    A variable maintains state in the graph across calls to run(). You add a variable to the graph by constructing an instance of the class Variable.
    The Variable() constructor requires an initial value for the variable, which can be a Tensor of any type and shape. The initial value defines the type and shape of the variable. After construction, the type and shape of the variable are fixed. The value can be changed using one of the assign methods.
    If you want to change the shape of a variable later you have to use an assign Op with validate-shape=False.
    Just like any Tensor, variables created with Variable() can be used as inputs for other Ops in the graph. Additionally, all the operators overloaded for the Tensor class are carried over to variables, so you can also add nodes to the graph by just doing arithmetic on variables.
    \begin{lstlisting}[language=Python]
import tensorflow as tf
# Create a variable.
w = tf.Variable(<initial-value>, name=<optional-name>)
# Use the variable in the graph like any Tensor.
y = tf.matmul(w, ...another variable or tensor...)
# The overloaded operators are available too.
z = tf.sigmoid(w + y)
# Assign a new value to the variable with `assign()` or a related method.
w.assign(w + 1.0)
w.assign_add(1.0)
\end{lstlisting}
\paragraph{Constants}
Creates a constant tensor.
The resulting tensor is populated with values of type dtype, as specified by arguments value and (optionally) shape .
The argument value can be a constant value, or a list of values of type dtype. If value is a list, then the length of the list must be less than or equal to the number of elements implied by the shape argument (if specified). In the case where the list length is less than the number of elements specified by shape, the last element in the list will be used to fill the remaining entries.
The argument shape is optional. If present, it specifies the dimensions of the resulting tensor. If not present, the shape of value is used.
If the argument dtype is not specified, then the type is inferred from the type of value.
\begin{lstlisting}[language=Python]
# Constant 1-D Tensor populated with value list.
tensor = tf.constant([1, 2, 3, 4, 5, 6, 7]) => [1 2 3 4 5 6 7]
# Constant 2-D tensor populated with scalar value -1.
tensor = tf.constant(-1.0, shape=[2, 3]) => [[-1. -1. -1.]
                                             [-1. -1. -1.]]

\end{lstlisting}
\paragraph{Placeholder}
So far we have used Variables to manage our data, but there is a more basic structure, the placeholder. A placeholder is simply a variable that we will assign data to at a later date. It allows us to create our operations and build our computation graph, without needing the data. In TensorFlow terminology, we then feed data into the graph through these placeholders.
This tensor will produce an error if evaluated. Its value must be fed using the \textbf{feed-dict} optional argument to \textbf{Session.run()}, \textbf{Tensor.eval()}, or \textbf{Operation.run()}.
\begin{lstlisting}[language=Python]
x = tf.placeholder(tf.float32, shape=(1024, 1024))
y = tf.matmul(x, x)
with tf.Session() as sess:
  print(sess.run(y))  # ERROR: will fail because x was not fed.
  rand_array = np.random.rand(1024, 1024)
  print(sess.run(y, feed_dict={x: rand_array}))  # Will succeed.
\end{lstlisting}
    \subsubsection{TensorFlow operations}
    
    \begin{table}[H] \centering  

\begin{tabular}{ |p{7cm}||p{9cm} |p{3cm}|p{3cm}| }
 \hline
 \multicolumn{2}{|c|}{Operations} \\
 \hline
 
 Category & Examples \\
 \hline
 Mathematical operations & Add, Sub, Mul, Div, Exp, Log,...\\
 Array operations & Concat, Slice, Split, Constant, Rank, Shape,...\\
 Matrix operations & MatMul, MatrixInverse, MatrixDeterminant,...\\
 Stateful operations & Variable, Assign, AssignAdd,...\\
 Neural Network building blocks & SoftMax, Sigmoid, ReLU, Convolution2D, MaxPool,...\\
 Checkpointing operations & Save, Reestore\\
 Queue and synchronization operations & Enqueue, Dequeue, MutexAcquire, MutexRelease,...\\
 Control flow operations & Merge, Switch, Enter, Leave, NextIteration\\
 
 
 \hline
 
\end{tabular}
\caption{Operations of Tensorflow}
\label{table:1}

\end{table}

\subsection{Graphs and Sessions}
TensorFlow uses a dataflow graph to represent your computation in terms of the dependencies between individual operations. This leads to a low-level programming model in which you first define the dataflow graph, then create a TensorFlow session to run parts of the graph across a set of local and remote devices.\\
\begin{figure}[H]%
    \center%
    \includegraphics[width=0.5\textwidth]{images/eman/Dataflow-graph.jpg}%
     % you need to add the caption for the list of figures
\caption[This is Dataflow Graph]{Dataflow Graph}\label{fig:graph}%
  \end{figure}
  
  \textbf{Why Dataflow Graphs?}\\
Dataflow is a common programming model for parallel computing. In a dataflow graph, the nodes represent units of computation, and the edges represent the data consumed or produced by a computation. For example, in a TensorFlow graph, the tf.matmul operation would correspond to a single node with two incoming edges (the matrices to be multiplied) and one outgoing edge (the result of the multiplication).\\
Dataflow has several advantages that TensorFlow leverages when executing your programs:
\begin{enumerate}
      \item\textcite{Parallelism.}
\item\textcite{Distributed execution.}
\item\textcite{Compilation.}
\item\textcite{Portability.}
\end{enumerate}

\paragraph{Graphs}
A \textbf{tf.Graph} contains two relevant kinds of information:
\begin{enumerate}
      \item\textcite{Graph structure}\\
      The nodes and edges of the graph, indicating how individual operations are composed together, but not prescribing how they should be used. The graph structure is like assembly code: inspecting it can convey some useful information, but it does not contain all of the useful context that source code conveys.
      
      \item\textcite{Graph collections}\\
      TensorFlow provides a general mechanism for storing collections of metadata in a \textbf{tf.Graph.} The \textbf{tf.add-to-collection} function enables you to associate a list of objects with a key (where \textbf{tf.GraphKeys} defines some of the standard keys), and \textbf{tf.get-collection} enables you to look up all objects associated with a key. Many parts of the TensorFlow library use this facility: for example, when you create a \textbf{tf.Variable}, it is added by default to collections representing "global variables" and "trainable variables". When you later come to create a \textbf{tf.train.Saver} or \textbf{tf.train.Optimizer}, the variables in these collections are used as the default arguments.
      
\end{enumerate}

\paragraph{Sessions}
    TensorFlow uses the \textbf{tf.Session} class to represent a connection between the client program ---typically a Python program, although a similar interface is available in other languages--- and the C++ runtime. A \textbf{tf.Session} object provides access to devices in the local machine, and remote devices using the distributed TensorFlow runtime. It also caches information about your \textbf{tf.Graph} so that you can efficiently run the same computation multiple times.\\
    \begin{lstlisting}[language=Python]
# Create a default in-process session.
with tf.Session() as sess:
  # ...
# Create a remote session.
with tf.Session("grpc://example.org:2222"):
  # ...

\end{lstlisting}

    Since a \textbf{tf.Session} owns physical resources (such as GPUs and network connections), it is typically used as a context manager (in a with block) that automatically closes the session when you exit the block. It is also possible to create a session without using a with block, but you should explicitly call \textbf{tf.Session.close} when you are finished with it to free the resources.\\
    \textbf{Note: }In Tensorflow you have an execution graph, which takes inputs and produces outputs. Naturally you \textbf{feed} the inputs into the graph, run it, and then \textbf{fetch} the outputs back into your program.\\
    \subsection{Save and restore models}
    \subsubsection{Meta graph}
    This is a protocol buffer which saves the complete Tensorflow graph; i.e. all variables, operations, collections etc. This file has .meta extension.
    \subsubsection{Checkpoints}
    TensorFlow provides two model formats:
    \begin{enumerate}
      \item\textcite{checkpoints}\\
      which is a format dependent on the code that created the model.
    \item\textcite{SavedModel}\\ 
    which is a format independent of the code that created the model.
    \end{enumerate}
Here, we will focus on checkpoints.\\
\textbf{Saving partially-trained models}\\
Estimators automatically write the following to disk:
\begin{enumerate}
      \item\textcite{checkpoints, }which are versions of the model created during training.
      \item\textcite{event files, }which contain information that TensorBoard uses to create visualizations.
      \end{enumerate}
      To specify the top-level directory in which the Estimator stores its information, assign a value to the optional model-dir argument of any Estimator's constructor. For example, the following code sets the model-dir argument to the models/iris directory:
      \begin{lstlisting}[language=Python]
classifier = tf.estimator.DNNClassifier(
    feature_columns=my_feature_columns,
    hidden_units=[10, 10],
    n_classes=3,
    model_dir='models/iris')

\end{lstlisting}
Suppose you call the Estimator's train method. For example:
\begin{lstlisting}[language=Python]
cclassifier.train(
        input_fn=lambda:train_input_fn(train_x, train_y, batch_size=100),
                steps=200)

\end{lstlisting}
As suggested by the following diagrams, the first call to train adds checkpoints and other files to the model-dir directory:
\begin{figure}[H]%
    \center%
    \includegraphics[width=0.8\textwidth]{images/eman/first_train_calls.png}%
     % you need to add the caption for the list of figures
\caption[This is first train call]{The first call to train()}\label{fig:call}%
  \end{figure}



 
  


      
  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
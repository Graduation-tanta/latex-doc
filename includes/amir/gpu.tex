\section{GPU and Specialized Hardware}
\label{chap:GPU and Specialized Hardware}

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % SUBSECTION               %
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Graphics processing unit}
GPU is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. GPUs are used in embedded systems, mobile phones, personal computers, workstations, and game consoles. Modern GPUs are very efficient at manipulating computer graphics and image processing, and their highly parallel structure makes them more efficient than general-purpose CPUs for algorithms where the processing of large blocks of data is done in parallel like usage in Deep Learning. In a personal computer, a GPU can be present on a video card or it can be embedded on the motherboard.\cite{web008}
\subsection{nVIDIA}
The term GPU was popularized by Nvidia in 1999; nVIDIA ,is American technology company, designs graphics processing units (GPUs) for the gaming and professional markets, as well as system on a chip units (SoCs) for the mobile computing and automotive market.Nvidia provides parallel processing capabilities to researchers and scientists that allow them to efficiently run high-performance applications.\cite{web009} \\
Nvidia GPUs are used in deep learning, artificial intelligence, and accelerated analytics.The company developed GPU-based deep learning in order to use AI to approach problems like cancer detection, weather prediction, and self-driving vehicles.They are included in all Tesla vehicles. Nvidia GPUs work well for deep learning tasks because they are designed for parallel computing and do well to handle the vector and matrix operations that are prevalent in deep learning.These GPUs are used by researchers, laboratories, tech companies and enterprise companies.\\
In 2009, Nvidia was involved in what was called the "big bang" of deep learning, "as deep-learning neural networks were combined with Nvidia graphics processing units (GPUs)".That year, the Google Brain used Nvidia GPUs to create Deep Neural Networks capable of machine learning, where Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times.\\
It also developed Nvidia Tesla K80 and P100 GPU-based virtual machines, which are available through Google Cloud, which Google installed in 2016. Microsoft added GPU servers in a preview offering of its N series based on Nvidia's Tesla K80s, each containing 4992 processing cores. Nvidia also partnered with IBM to create a software kit that boosts the AI capabilities of Watson, called IBM PowerAI. Nvidia also offers its own NVIDIA Deep Learning software development kit.\\ In May 2018, researchers at the artificial intelligence department of NVidia realized the possibility that a robot can learn to perform a job simply by observing the person doing the same job.They have created a system that, after a short revision and testing, can already be used to control the universal robots of the next generation.
\subsubsection{GPU Technology Conference}
NVIDIAâ€™s GPU Technology Conference (GTC) is a series of technical conferences held around the world. It originated in 2009 in San Jose, CA with an initial focus on the potential for solving computing challenges through GPUs. In recent years, the conference focus has shifted to various applications of AI and Data Science, including: self-driving cars, healthcare, high performance computing, and NVIDIA Deep Learning Institute (DLI) training.
\subsection{Hardware Acceleration}
In computing, hardware acceleration is the use of computer hardware to perform some functions more efficiently than is possible in software running on a more general-purpose CPU. Examples of hardware acceleration include Bit blit acceleration functionality in (GPUs) and regular expression hardware acceleration for spam control in the server industry.The hardware that performs the acceleration may be part of a general-purpose CPU, or a separate unit. It is referred to as a hardware accelerator, or often more specifically as a 3D accelerator, cryptographic accelerator, etc.\cite{web010}\\
Hardware accelerators improve the execution of a specific algorithm by allowing greater concurrency, having specific data-paths for its temporaries, and possibly reducing the overhead of instruction control.
Hardware acceleration is suitable for any repetitive, intensive key algorithm.\\
Examples: GPUs, Application-specific integrated circuit, Field-programmable gate array (FPGAs), Cryptographic accelerator, VPUs.,
AI accelerator, Regular expression accelerator, and Data compression accelerator.